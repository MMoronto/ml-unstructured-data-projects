{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tile-based classification using Sentinel-2 and EuroSAT data-Training.ipynb",
      "provenance": [],
      "mount_file_id": "1Nb6axbd2makj-lAkYy_81CqLwf8aKOQ9",
      "authorship_tag": "ABX9TyPwdyNE7v9NFjktXbHHw9oF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMoronto/ml-unstructured-data-projects/blob/master/Tile_based_classification_using_Sentinel_2_and_EuroSAT_data_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeFaI7puszgm"
      },
      "source": [
        "##**Intro**\n",
        "\n",
        "This workflow explores the process of training a `Convolutional Neural Network (CNN)` with Keras based on the benchmark dataset EuroSAT. This noote book contains notes I've taken as I work through the example workflow presented in the AI for Earth monitoring MOOC on Futurelearn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkSeuefswyfX"
      },
      "source": [
        "##**Machine-Learning Algorithm**\n",
        "\n",
        "This example develops a `Sequential Convolutional Neural Network (CNN)` with TF Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyS5FFHUxcN_"
      },
      "source": [
        "##**Data**\n",
        "The model is trained on the EuroSAT benchmark dataset which is based on Sentinel-2 satellite images and consists of 27,000 labeled and geo-referenced images.The dataset provides information on the following ten land cover/land use cases:\n",
        "* `Annual Crop`\n",
        "* `Forest`\n",
        "* `Herbaceous Vegetation`\n",
        "* `Highway`\n",
        "* Industrial`\n",
        "* `Pasture`\n",
        "* `Permanent Crop`\n",
        "* `Residential`\n",
        "* `River`\n",
        "* `Sea Lake`\n",
        "\n",
        "The benchmark dataset can be used to detect `land cover / land use changes`. The geo-referenced datasset EuroSAT is publicly accessible here: https://github.com/phelber/eurosat\n",
        "\n",
        "## Notebook Outline\n",
        "* 1 - Load the EuroSAT benchmark dataset as input data\n",
        "* 2 - Create training and test subsets from input data\n",
        "* 3 - Define the Convolutional Neural Network architecture\n",
        "* 4 - Fit (train) the convolutional neural network (CNN)\n",
        "* 5 - Evaluate the performance of the CNN model with a confusion matrix\n",
        "\n",
        "\n",
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsppuKATLSor"
      },
      "source": [
        "## Begin S3FS Import Snippet ##\n",
        "import os, sys\n",
        "s3_home = os.getcwd()\n",
        "try: sys.path.remove(s3_home) # Remove the S3 from the $path\n",
        "except Exception: pass\n",
        "\n",
        "current_dir = os.get_cwd()\n",
        "\n",
        "os.chdir('/home/jovyan') # Temporarily move to another directory\n",
        "\n",
        "# Begin imports #\n",
        "from tensorflow import Keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from osgeo import gdal_arrar\n",
        "from matplotlib import pyplot as pyplot\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import zipfile\n",
        "# end imports #\n",
        "\n",
        "os.chdir(current_dir) # go back to your previous dir\n",
        "\n",
        "sys.path.append(s3_home) # restore the s3 root in the $path\n",
        "\n",
        "## end s3fs import snippet ##\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttI_pu8vzd8_"
      },
      "source": [
        "# !unzip \"drive/MyDrive/Land Classification/S2_Tile_based_classification.zip\" -d \"drive/MyDrive/Land Classification/\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-DF0lwEWyWk"
      },
      "source": [
        "Define helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3mvsK6CKOO-"
      },
      "source": [
        "from_folder_to_stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwNtHfldXIaf"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "'''\n",
        "function name:\n",
        "  from_folder_to_stack\n",
        "description:\n",
        "  This function transforms the .SAFE file into three different arrays (10m, 20m and 60m).\n",
        "Input:\n",
        "  safe_path: the path of the .SAFE file;\n",
        "  data_bands_20m: if True, the fumnction computes stack using Sentinel2 band with 20m of pixel resolution (default=True);\n",
        "  data_bands_60m: if True, the function computes stack using Sentinel2 band with 60m of pixel resolution (default=True);\n",
        "Output:\n",
        "  stack_10m: stack with the following S2L1C bands (B02, B03, B04, B08)\n",
        "  stack_20m: stack with the following S2L1C bands (B05, B06, B07, B11, B12, B8A)\n",
        "  stack_60m: stack with the following S2L1C bands (B01, mB09, B10)\n",
        "'''\n",
        "def from_folder_to_stack(\n",
        "    safe_path,\n",
        "    data_bands_20m=True,\n",
        "    data_bands_60m=True,\n",
        "    ):\n",
        "\n",
        "  level_folder_name_list = glob.glob(safe_path + 'GRANULE/*')\n",
        "  level_folder_name = level_folder_name_list[0]\n",
        "\n",
        "  if level_folder_name.find(\"L2A\") < 0:\n",
        "    safe_path = [level_folder_name + '/IMG_DATA/']\n",
        "  else:\n",
        "    safe_path_10m = level_folder_name + '/IMG_DATA/R10m/'\n",
        "    safe_path = [safe_path_10m]\n",
        "\n",
        "  text_files = []\n",
        "\n",
        "  for i in range(0, len(safe_path)):\n",
        "      print(\"[AI4EO_MOOC]_log: Loading .jp2 images in %s\" % (safe_path[i]))\n",
        "      text_files_tmp = [f for f in os.listdir(safe_path[i]) if f.endswith('.jp2')]\n",
        "      text_files.append(text_files_tmp)\n",
        "\n",
        "  lst_stack_60m=[]\n",
        "  lst_code_60m=[]\n",
        "  lst_stack_20m=[]\n",
        "  lst_code_20m=[]\n",
        "  lst_stack_10m=[]\n",
        "  lst_code_10m=[]\n",
        "  for i in range(0, len(safe_path)):\n",
        "\n",
        "    print(\"[AI4EO_MOOC]_log: Reading .jp2 files in %s\" % (safe_path[i]))\n",
        "    for name in range(0, len(text_files[i])):\n",
        "      text_files_tmp = text_files[i]\n",
        "      if data_bands_60m == True:\n",
        "        cond_60m = ( (text_files_tmp[name].find(\"B01\") > 0) or (text_files_tmp[name].find(\"B09\") > 0)\n",
        "                    or (text_files_tmp[name].find(\"B10\") > 0))\n",
        "        if cond_60m:\n",
        "            print(\"[AI4EO_MOOC]_log: Using .jp2 image: %s\" % text_files_tmp[name])\n",
        "            lst_stack_60m.append(gdal_array.LoadFile(safe_path[i] + text_files_tmp[name]))\n",
        "            lst_code_60m.append(text_files_tmp[name][24:26])\n",
        "\n",
        "      if data_bands_20m == True:\n",
        "          cond_20m = (text_files_tmp[name].find(\"B05\") > 0) or (text_files_tmp[name].find(\"B06\") > 0) or (\n",
        "                      text_files_tmp[name].find(\"B07\") > 0) or (text_files_tmp[name].find(\"B11\") > 0) or (\n",
        "                                  text_files_tmp[name].find(\"B12\") > 0) or (text_files_tmp[name].find(\"B8A\") > 0)\n",
        "          cond_60m_L2 = (text_files_tmp[name].find(\"B05_60m\") < 0) and (text_files_tmp[name].find(\"B06_60m\") < 0) and (\n",
        "                      text_files_tmp[name].find(\"B07_60m\") < 0) and (text_files_tmp[name].find(\"B11_60m\") < 0) and (\n",
        "                                  text_files_tmp[name].find(\"B12_60m\") < 0) and (text_files_tmp[name].find(\"B8A_60m\") < 0)\n",
        "          cond_20m_tot = cond_20m and cond_60m_L2\n",
        "          if cond_20m_tot:\n",
        "              print(\"[AI4E_MOOC]_log: Using .jp2 image: %s\" % text_files_tmp[name])\n",
        "              lst_stack_20m.append(gdal_array.LoadFile(safe_path[i] + text_files_tmp[name]))\n",
        "              lst_code_20m.append(text_files_tmp[name][24:26])\n",
        "      else:\n",
        "        stack_20m = 0\n",
        "\n",
        "      cond_10m = (text_files_tmp[name].find(\"B02\") > 0) or (text_files_tmp[name].find(\"B03\") > 0) or (\n",
        "                  text_files_tmp[name].find(\"B04\") > 0) or (text_files_tmp[name].find(\"B08\") > 0)\n",
        "      cond_20m_L2 = (text_files_tmp[name].find(\"B02_20m\") < 0) and (text_files_tmp[name].find(\"B03_20m\") < 0) and (\n",
        "                  text_files_tmp[name].find(\"B04_20m\") < 0) and (text_files_tmp[name].find(\"B08_20m\") < 0)\n",
        "      cond_60m_L2 = (text_files_tmp[name].find(\"B02_60m\") < 0) and (text_files_tmp[name].find(\"B03_60m\") < 0) and (\n",
        "                  text_files_tmp[name].find(\"B04_60m\") < 0) and (text_files_tmp[name].find(\"B08_60m\") < 0)\n",
        "      cond_10m_tot = cond_10m and cond_20m_L2 and cond_60m_L2\n",
        "\n",
        "      if cond_10m_tot:\n",
        "          print(\"[AI4E)_MOOC]_log: Using .jp2 image: %s\" % text_files_tmp[name])\n",
        "          lst_stack_10m.append(gdal_array.LoadFile(safe_path[i] + text_files_tmp[name]))\n",
        "          lst_code_10m.append(text_files_tmp[name][24:26])\n",
        "\n",
        "\n",
        "  stack_10m=np.asarray(lst_stack_10m)\n",
        "  sorted_list_10m = ['02', '03', '04', '08']\n",
        "  print('[AI4EO_MOOC]_log: Sorting stack 10m...')\n",
        "  stack_10m_final_sorted = stack_sort(stack_10m, lst_code_10m, sorted_list_10m)\n",
        "\n",
        "  stack_20m=np.asarray(lst_stack_20m)\n",
        "  sorted_list_20m = ['05', '06', '07', '11', '12', '8A']\n",
        "  print('[AI4EO_MOOC]_log: Sorting stack 20m...')\n",
        "  stack_20m_final_sorted = stack_sort(stack_20m, lst_code_20m, sorted_list_20m)\n",
        "              \n",
        "  stack_60m=np.asarray(lst_stack_60m)\n",
        "  sorted_list_60m = ['01', '09', '10']\n",
        "  print('[AI4EO_MOOC]_log: Sorting stack 60m...')\n",
        "  stack_60m_final_sorted = stack_sort(stack_60m, lst_code_60m, sorted_list_60m)\n",
        "\n",
        "  return stack_10m_final_sorted, stack_20m_final_sorted, stack_60m_final_sorted"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwSv2ExkGK9e"
      },
      "source": [
        "stack_sort"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW6E0LvU_wB0"
      },
      "source": [
        "def stack_sort(stack_in, lst_code, sorted_list):\n",
        "  b, r, c = stack_in.shape\n",
        "  stack_sorted = np.zeros((r,c,b), dtype=np.unit16)\n",
        "\n",
        "  len_list_bands = len(lst_code)\n",
        "\n",
        "  c = np.zeros((len_list_bands), dtype=np.unit8)\n",
        "  count = 0\n",
        "  count_sort = 0\n",
        "  while count_sort != len_list_bands:\n",
        "    if lst_code[count] == sorted_list[count_sort]:\n",
        "      c[count_sort] = count\n",
        "      count_sort = count_sort + 1\n",
        "      count = 0\n",
        "    else:\n",
        "      count = count + 1\n",
        "    print('[AI4EO_MOOC]_log: sorted list:', sorted_list)\n",
        "    print('[AI4EO_MOOC]_log: bands:', c)\n",
        "    for i in range(0, len_list_bands):\n",
        "        stack_sorted[:,:,i]=stack_in[c[i],:,:]\n",
        "    \n",
        "    return stack_sorted"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ncaoujg8H-7h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}